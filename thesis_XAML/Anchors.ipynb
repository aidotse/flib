{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.ensemble\n",
    "import numpy as np\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import pandas as pd\n",
    "np.random.seed(1)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "import sys\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "import anchor\n",
    "from anchor import utils\n",
    "from anchor import anchor_tabular\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iris = sklearn.datasets.load_iris()\n",
    "features = iris.data\n",
    "labels = iris.target\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(features[labels == 0,2], features[labels == 0, 3], c = 'r', label = 'Setosa')\n",
    "plt.scatter(features[labels == 1,2], features[labels == 1, 3], c = 'g', label = 'Versicolor')\n",
    "plt.scatter(features[labels == 2,2], features[labels == 2, 3], c = 'b', label = 'Virginica')\n",
    "plt.show()\n",
    "\n",
    "features = iris.data\n",
    "labels = iris.target\n",
    "\n",
    "features = features[labels != 0] # Drop class 0\n",
    "features = features[:,2:] # Drop features 0 and 1\n",
    "\n",
    "labels = labels[labels != 0] # Drop class 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels\n",
    "\n",
    "\n",
    "\n",
    "#replace labels==2 with 0\n",
    "labels[labels == 2] = 0\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, labels_train, labels_test = sklearn.model_selection.train_test_split(features, labels, train_size=0.80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=sklearn.linear_model.LogisticRegression()\n",
    "c.fit(train, labels_train)\n",
    "predict=c.predict(test)\n",
    "predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['virginica', 'versicolor'], dtype='<U10')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = iris.feature_names[2:]\n",
    "feature_names\n",
    "target_names = iris.target_names[1:][::-1]\n",
    "target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = anchor_tabular.AnchorTabularExplainer(\n",
    "    target_names,\n",
    "    feature_names,\n",
    "    train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  virginica\n"
     ]
    }
   ],
   "source": [
    "idx=0\n",
    "print('Prediction: ', explainer.class_names[c.predict(test[idx].reshape(1, -1))[0]])\n",
    "exp = explainer.explain_instance(test[idx], c.predict, threshold=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: petal length (cm) > 5.50\n",
      "Precision: 1.00\n",
      "Coverage: 0.22\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Anchor: %s' % (' AND '.join(exp.names())))\n",
    "print('Precision: %.2f' % exp.precision())\n",
    "print('Coverage: %.2f' % exp.coverage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test, labels_train, labels_test = sklearn.model_selection.train_test_split(features, labels, train_size=0.80)\n",
    "train_np=train\n",
    "test_np=test\n",
    "\n",
    "train = torch.from_numpy(train)\n",
    "test = torch.from_numpy(test)\n",
    "\n",
    "labels_train = torch.from_numpy(labels_train).reshape(-1,1)\n",
    "labels_test = torch.from_numpy(labels_test).reshape(-1,1)\n",
    "\n",
    "\n",
    "labels_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressor(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegressor, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def class_probabilities(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.forward(x)\n",
    "            class_probs = torch.cat((1-x, x), dim = 1)\n",
    "        return class_probs.reshape(-1,2)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.forward(x)\n",
    "            predicted_class = x.detach().round()\n",
    "        return predicted_class.reshape(-1,1)\n",
    "\n",
    "model = LogisticRegressor(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Loss: 1.1693434715270996 Training accuracy = 0.550000011920929 | Test accuracy = 0.30000001192092896\n",
      "Epoch [2/500], Loss: 1.0432627201080322 Training accuracy = 0.550000011920929 | Test accuracy = 0.30000001192092896\n",
      "Epoch [3/500], Loss: 0.9258748888969421 Training accuracy = 0.550000011920929 | Test accuracy = 0.30000001192092896\n",
      "Epoch [4/500], Loss: 0.8204602003097534 Training accuracy = 0.550000011920929 | Test accuracy = 0.30000001192092896\n",
      "Epoch [5/500], Loss: 0.7310886979103088 Training accuracy = 0.550000011920929 | Test accuracy = 0.30000001192092896\n",
      "Epoch [6/500], Loss: 0.662301242351532 Training accuracy = 0.550000011920929 | Test accuracy = 0.30000001192092896\n",
      "Epoch [7/500], Loss: 0.6181621551513672 Training accuracy = 0.5874999761581421 | Test accuracy = 0.30000001192092896\n",
      "Epoch [8/500], Loss: 0.6004412770271301 Training accuracy = 0.75 | Test accuracy = 0.6000000238418579\n",
      "Epoch [9/500], Loss: 0.6063541769981384 Training accuracy = 0.8500000238418579 | Test accuracy = 0.8999999761581421\n",
      "Epoch [10/500], Loss: 0.6276497840881348 Training accuracy = 0.5249999761581421 | Test accuracy = 0.699999988079071\n",
      "Epoch [11/500], Loss: 0.6530269384384155 Training accuracy = 0.44999998807907104 | Test accuracy = 0.699999988079071\n",
      "Epoch [12/500], Loss: 0.6727248430252075 Training accuracy = 0.44999998807907104 | Test accuracy = 0.699999988079071\n",
      "Epoch [13/500], Loss: 0.6815180778503418 Training accuracy = 0.44999998807907104 | Test accuracy = 0.699999988079071\n",
      "Epoch [14/500], Loss: 0.6786898970603943 Training accuracy = 0.4625000059604645 | Test accuracy = 0.699999988079071\n",
      "Epoch [15/500], Loss: 0.666496992111206 Training accuracy = 0.5249999761581421 | Test accuracy = 0.699999988079071\n",
      "Epoch [16/500], Loss: 0.6486142873764038 Training accuracy = 0.75 | Test accuracy = 0.800000011920929\n",
      "Epoch [17/500], Loss: 0.6290037035942078 Training accuracy = 0.9375 | Test accuracy = 1.0\n",
      "Epoch [18/500], Loss: 0.6111418008804321 Training accuracy = 0.824999988079071 | Test accuracy = 0.6499999761581421\n",
      "Epoch [19/500], Loss: 0.5974999070167542 Training accuracy = 0.699999988079071 | Test accuracy = 0.4000000059604645\n",
      "Epoch [20/500], Loss: 0.5892643332481384 Training accuracy = 0.6000000238418579 | Test accuracy = 0.4000000059604645\n",
      "Epoch [21/500], Loss: 0.5863296389579773 Training accuracy = 0.5874999761581421 | Test accuracy = 0.30000001192092896\n",
      "Epoch [22/500], Loss: 0.5875620245933533 Training accuracy = 0.5625 | Test accuracy = 0.30000001192092896\n",
      "Epoch [23/500], Loss: 0.5912471413612366 Training accuracy = 0.550000011920929 | Test accuracy = 0.30000001192092896\n",
      "Epoch [24/500], Loss: 0.5955724716186523 Training accuracy = 0.550000011920929 | Test accuracy = 0.30000001192092896\n",
      "Epoch [25/500], Loss: 0.5990149974822998 Training accuracy = 0.550000011920929 | Test accuracy = 0.30000001192092896\n",
      "Epoch [26/500], Loss: 0.6005643606185913 Training accuracy = 0.550000011920929 | Test accuracy = 0.30000001192092896\n",
      "Epoch [27/500], Loss: 0.5997887849807739 Training accuracy = 0.550000011920929 | Test accuracy = 0.30000001192092896\n",
      "Epoch [28/500], Loss: 0.5967836380004883 Training accuracy = 0.550000011920929 | Test accuracy = 0.30000001192092896\n",
      "Epoch [29/500], Loss: 0.5920513868331909 Training accuracy = 0.5625 | Test accuracy = 0.30000001192092896\n",
      "Epoch [30/500], Loss: 0.5863469839096069 Training accuracy = 0.5874999761581421 | Test accuracy = 0.30000001192092896\n",
      "Epoch [31/500], Loss: 0.5805128812789917 Training accuracy = 0.6000000238418579 | Test accuracy = 0.3499999940395355\n",
      "Epoch [32/500], Loss: 0.5753175616264343 Training accuracy = 0.6000000238418579 | Test accuracy = 0.4000000059604645\n",
      "Epoch [33/500], Loss: 0.5713164210319519 Training accuracy = 0.6499999761581421 | Test accuracy = 0.4000000059604645\n",
      "Epoch [34/500], Loss: 0.5687578320503235 Training accuracy = 0.6875 | Test accuracy = 0.4000000059604645\n",
      "Epoch [35/500], Loss: 0.5675536394119263 Training accuracy = 0.75 | Test accuracy = 0.6000000238418579\n",
      "Epoch [36/500], Loss: 0.5673278570175171 Training accuracy = 0.800000011920929 | Test accuracy = 0.6000000238418579\n",
      "Epoch [37/500], Loss: 0.5675336718559265 Training accuracy = 0.8374999761581421 | Test accuracy = 0.6499999761581421\n",
      "Epoch [38/500], Loss: 0.5676054358482361 Training accuracy = 0.8500000238418579 | Test accuracy = 0.699999988079071\n",
      "Epoch [39/500], Loss: 0.5671022534370422 Training accuracy = 0.8500000238418579 | Test accuracy = 0.699999988079071\n",
      "Epoch [40/500], Loss: 0.5658013224601746 Training accuracy = 0.8500000238418579 | Test accuracy = 0.699999988079071\n",
      "Epoch [41/500], Loss: 0.5637210607528687 Training accuracy = 0.8500000238418579 | Test accuracy = 0.6499999761581421\n",
      "Epoch [42/500], Loss: 0.5610774755477905 Training accuracy = 0.800000011920929 | Test accuracy = 0.6000000238418579\n",
      "Epoch [43/500], Loss: 0.5581954121589661 Training accuracy = 0.7875000238418579 | Test accuracy = 0.6000000238418579\n",
      "Epoch [44/500], Loss: 0.5554076433181763 Training accuracy = 0.75 | Test accuracy = 0.6000000238418579\n",
      "Epoch [45/500], Loss: 0.5529677271842957 Training accuracy = 0.7250000238418579 | Test accuracy = 0.44999998807907104\n",
      "Epoch [46/500], Loss: 0.5509995818138123 Training accuracy = 0.6875 | Test accuracy = 0.4000000059604645\n",
      "Epoch [47/500], Loss: 0.5494884252548218 Training accuracy = 0.6875 | Test accuracy = 0.4000000059604645\n",
      "Epoch [48/500], Loss: 0.5483094453811646 Training accuracy = 0.675000011920929 | Test accuracy = 0.4000000059604645\n",
      "Epoch [49/500], Loss: 0.5472790002822876 Training accuracy = 0.675000011920929 | Test accuracy = 0.4000000059604645\n",
      "Epoch [50/500], Loss: 0.5462117195129395 Training accuracy = 0.675000011920929 | Test accuracy = 0.4000000059604645\n",
      "Epoch [51/500], Loss: 0.5449661612510681 Training accuracy = 0.675000011920929 | Test accuracy = 0.4000000059604645\n",
      "Epoch [52/500], Loss: 0.5434728860855103 Training accuracy = 0.675000011920929 | Test accuracy = 0.4000000059604645\n",
      "Epoch [53/500], Loss: 0.5417391061782837 Training accuracy = 0.6875 | Test accuracy = 0.4000000059604645\n",
      "Epoch [54/500], Loss: 0.5398339033126831 Training accuracy = 0.6875 | Test accuracy = 0.4000000059604645\n",
      "Epoch [55/500], Loss: 0.5378606915473938 Training accuracy = 0.7250000238418579 | Test accuracy = 0.44999998807907104\n",
      "Epoch [56/500], Loss: 0.5359257459640503 Training accuracy = 0.737500011920929 | Test accuracy = 0.5\n",
      "Epoch [57/500], Loss: 0.5341091752052307 Training accuracy = 0.762499988079071 | Test accuracy = 0.6000000238418579\n",
      "Epoch [58/500], Loss: 0.5324476361274719 Training accuracy = 0.7875000238418579 | Test accuracy = 0.6000000238418579\n",
      "Epoch [59/500], Loss: 0.5309308767318726 Training accuracy = 0.800000011920929 | Test accuracy = 0.6000000238418579\n",
      "Epoch [60/500], Loss: 0.5295108556747437 Training accuracy = 0.8125 | Test accuracy = 0.6000000238418579\n",
      "Epoch [61/500], Loss: 0.528121292591095 Training accuracy = 0.824999988079071 | Test accuracy = 0.6000000238418579\n",
      "Epoch [62/500], Loss: 0.5266987681388855 Training accuracy = 0.824999988079071 | Test accuracy = 0.6000000238418579\n",
      "Epoch [63/500], Loss: 0.5252009630203247 Training accuracy = 0.824999988079071 | Test accuracy = 0.6499999761581421\n",
      "Epoch [64/500], Loss: 0.5236154794692993 Training accuracy = 0.824999988079071 | Test accuracy = 0.6499999761581421\n",
      "Epoch [65/500], Loss: 0.5219581723213196 Training accuracy = 0.824999988079071 | Test accuracy = 0.6000000238418579\n",
      "Epoch [66/500], Loss: 0.5202642679214478 Training accuracy = 0.824999988079071 | Test accuracy = 0.6000000238418579\n",
      "Epoch [67/500], Loss: 0.5185744166374207 Training accuracy = 0.824999988079071 | Test accuracy = 0.6000000238418579\n",
      "Epoch [68/500], Loss: 0.5169216990470886 Training accuracy = 0.8125 | Test accuracy = 0.6000000238418579\n",
      "Epoch [69/500], Loss: 0.5153232216835022 Training accuracy = 0.8125 | Test accuracy = 0.6000000238418579\n",
      "Epoch [70/500], Loss: 0.5137776136398315 Training accuracy = 0.8125 | Test accuracy = 0.6000000238418579\n",
      "Epoch [71/500], Loss: 0.5122693777084351 Training accuracy = 0.8125 | Test accuracy = 0.6000000238418579\n",
      "Epoch [72/500], Loss: 0.5107752084732056 Training accuracy = 0.8125 | Test accuracy = 0.6000000238418579\n",
      "Epoch [73/500], Loss: 0.5092729330062866 Training accuracy = 0.8125 | Test accuracy = 0.6000000238418579\n",
      "Epoch [74/500], Loss: 0.5077479481697083 Training accuracy = 0.8125 | Test accuracy = 0.6000000238418579\n",
      "Epoch [75/500], Loss: 0.5061959028244019 Training accuracy = 0.8125 | Test accuracy = 0.6000000238418579\n",
      "Epoch [76/500], Loss: 0.5046226382255554 Training accuracy = 0.824999988079071 | Test accuracy = 0.6000000238418579\n",
      "Epoch [77/500], Loss: 0.5030405521392822 Training accuracy = 0.824999988079071 | Test accuracy = 0.6000000238418579\n",
      "Epoch [78/500], Loss: 0.5014637112617493 Training accuracy = 0.824999988079071 | Test accuracy = 0.6499999761581421\n",
      "Epoch [79/500], Loss: 0.4999033510684967 Training accuracy = 0.824999988079071 | Test accuracy = 0.6499999761581421\n",
      "Epoch [80/500], Loss: 0.4983648657798767 Training accuracy = 0.824999988079071 | Test accuracy = 0.6499999761581421\n",
      "Epoch [81/500], Loss: 0.49684715270996094 Training accuracy = 0.8500000238418579 | Test accuracy = 0.6499999761581421\n",
      "Epoch [82/500], Loss: 0.4953438341617584 Training accuracy = 0.8500000238418579 | Test accuracy = 0.6499999761581421\n",
      "Epoch [83/500], Loss: 0.4938465654850006 Training accuracy = 0.8500000238418579 | Test accuracy = 0.6499999761581421\n",
      "Epoch [84/500], Loss: 0.4923478066921234 Training accuracy = 0.8500000238418579 | Test accuracy = 0.6499999761581421\n",
      "Epoch [85/500], Loss: 0.49084311723709106 Training accuracy = 0.8500000238418579 | Test accuracy = 0.6499999761581421\n",
      "Epoch [86/500], Loss: 0.4893323481082916 Training accuracy = 0.8500000238418579 | Test accuracy = 0.6499999761581421\n",
      "Epoch [87/500], Loss: 0.48781853914260864 Training accuracy = 0.8500000238418579 | Test accuracy = 0.6499999761581421\n",
      "Epoch [88/500], Loss: 0.4863068461418152 Training accuracy = 0.8500000238418579 | Test accuracy = 0.6499999761581421\n",
      "Epoch [89/500], Loss: 0.4848023056983948 Training accuracy = 0.8500000238418579 | Test accuracy = 0.6499999761581421\n",
      "Epoch [90/500], Loss: 0.4833081364631653 Training accuracy = 0.8500000238418579 | Test accuracy = 0.6499999761581421\n",
      "Epoch [91/500], Loss: 0.4818252921104431 Training accuracy = 0.8500000238418579 | Test accuracy = 0.6499999761581421\n",
      "Epoch [92/500], Loss: 0.48035240173339844 Training accuracy = 0.8500000238418579 | Test accuracy = 0.6499999761581421\n",
      "Epoch [93/500], Loss: 0.4788866937160492 Training accuracy = 0.8500000238418579 | Test accuracy = 0.6499999761581421\n",
      "Epoch [94/500], Loss: 0.4774252474308014 Training accuracy = 0.8500000238418579 | Test accuracy = 0.6499999761581421\n",
      "Epoch [95/500], Loss: 0.4759659171104431 Training accuracy = 0.8500000238418579 | Test accuracy = 0.6499999761581421\n",
      "Epoch [96/500], Loss: 0.47450822591781616 Training accuracy = 0.8500000238418579 | Test accuracy = 0.6499999761581421\n",
      "Epoch [97/500], Loss: 0.4730526804924011 Training accuracy = 0.862500011920929 | Test accuracy = 0.6499999761581421\n",
      "Epoch [98/500], Loss: 0.4716011881828308 Training accuracy = 0.862500011920929 | Test accuracy = 0.6499999761581421\n",
      "Epoch [99/500], Loss: 0.4701555669307709 Training accuracy = 0.862500011920929 | Test accuracy = 0.6499999761581421\n",
      "Epoch [100/500], Loss: 0.4687173366546631 Training accuracy = 0.875 | Test accuracy = 0.75\n",
      "Epoch [101/500], Loss: 0.46728697419166565 Training accuracy = 0.875 | Test accuracy = 0.75\n",
      "Epoch [102/500], Loss: 0.465864360332489 Training accuracy = 0.875 | Test accuracy = 0.75\n",
      "Epoch [103/500], Loss: 0.4644485414028168 Training accuracy = 0.875 | Test accuracy = 0.75\n",
      "Epoch [104/500], Loss: 0.46303844451904297 Training accuracy = 0.875 | Test accuracy = 0.800000011920929\n",
      "Epoch [105/500], Loss: 0.46163320541381836 Training accuracy = 0.875 | Test accuracy = 0.800000011920929\n",
      "Epoch [106/500], Loss: 0.4602324068546295 Training accuracy = 0.875 | Test accuracy = 0.800000011920929\n",
      "Epoch [107/500], Loss: 0.45883646607398987 Training accuracy = 0.875 | Test accuracy = 0.800000011920929\n",
      "Epoch [108/500], Loss: 0.45744577050209045 Training accuracy = 0.875 | Test accuracy = 0.800000011920929\n",
      "Epoch [109/500], Loss: 0.4560612142086029 Training accuracy = 0.875 | Test accuracy = 0.800000011920929\n",
      "Epoch [110/500], Loss: 0.45468324422836304 Training accuracy = 0.875 | Test accuracy = 0.800000011920929\n",
      "Epoch [111/500], Loss: 0.45331230759620667 Training accuracy = 0.875 | Test accuracy = 0.800000011920929\n",
      "Epoch [112/500], Loss: 0.45194825530052185 Training accuracy = 0.875 | Test accuracy = 0.800000011920929\n",
      "Epoch [113/500], Loss: 0.45059090852737427 Training accuracy = 0.875 | Test accuracy = 0.800000011920929\n",
      "Epoch [114/500], Loss: 0.44923973083496094 Training accuracy = 0.875 | Test accuracy = 0.800000011920929\n",
      "Epoch [115/500], Loss: 0.44789451360702515 Training accuracy = 0.875 | Test accuracy = 0.800000011920929\n",
      "Epoch [116/500], Loss: 0.44655507802963257 Training accuracy = 0.875 | Test accuracy = 0.800000011920929\n",
      "Epoch [117/500], Loss: 0.44522160291671753 Training accuracy = 0.875 | Test accuracy = 0.800000011920929\n",
      "Epoch [118/500], Loss: 0.44389429688453674 Training accuracy = 0.875 | Test accuracy = 0.800000011920929\n",
      "Epoch [119/500], Loss: 0.4425733685493469 Training accuracy = 0.875 | Test accuracy = 0.800000011920929\n",
      "Epoch [120/500], Loss: 0.4412590563297272 Training accuracy = 0.875 | Test accuracy = 0.800000011920929\n",
      "Epoch [121/500], Loss: 0.43995141983032227 Training accuracy = 0.875 | Test accuracy = 0.800000011920929\n",
      "Epoch [122/500], Loss: 0.438650518655777 Training accuracy = 0.875 | Test accuracy = 0.800000011920929\n",
      "Epoch [123/500], Loss: 0.43735623359680176 Training accuracy = 0.875 | Test accuracy = 0.800000011920929\n",
      "Epoch [124/500], Loss: 0.4360683858394623 Training accuracy = 0.8999999761581421 | Test accuracy = 0.949999988079071\n",
      "Epoch [125/500], Loss: 0.4347867965698242 Training accuracy = 0.8999999761581421 | Test accuracy = 0.949999988079071\n",
      "Epoch [126/500], Loss: 0.43351155519485474 Training accuracy = 0.8999999761581421 | Test accuracy = 0.949999988079071\n",
      "Epoch [127/500], Loss: 0.43224263191223145 Training accuracy = 0.8999999761581421 | Test accuracy = 0.949999988079071\n",
      "Epoch [128/500], Loss: 0.4309801161289215 Training accuracy = 0.8999999761581421 | Test accuracy = 0.949999988079071\n",
      "Epoch [129/500], Loss: 0.42972415685653687 Training accuracy = 0.8999999761581421 | Test accuracy = 0.949999988079071\n",
      "Epoch [130/500], Loss: 0.4284747242927551 Training accuracy = 0.8999999761581421 | Test accuracy = 0.949999988079071\n",
      "Epoch [131/500], Loss: 0.42723196744918823 Training accuracy = 0.8999999761581421 | Test accuracy = 0.949999988079071\n",
      "Epoch [132/500], Loss: 0.42599573731422424 Training accuracy = 0.8999999761581421 | Test accuracy = 0.949999988079071\n",
      "Epoch [133/500], Loss: 0.4247661232948303 Training accuracy = 0.8999999761581421 | Test accuracy = 0.949999988079071\n",
      "Epoch [134/500], Loss: 0.42354291677474976 Training accuracy = 0.8999999761581421 | Test accuracy = 0.949999988079071\n",
      "Epoch [135/500], Loss: 0.4223262667655945 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [136/500], Loss: 0.4211159646511078 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [137/500], Loss: 0.419912189245224 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [138/500], Loss: 0.4187147617340088 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [139/500], Loss: 0.41752395033836365 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [140/500], Loss: 0.41633957624435425 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [141/500], Loss: 0.41516175866127014 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [142/500], Loss: 0.41399040818214417 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [143/500], Loss: 0.41282549500465393 Training accuracy = 0.925000011920929 | Test accuracy = 0.949999988079071\n",
      "Epoch [144/500], Loss: 0.41166695952415466 Training accuracy = 0.925000011920929 | Test accuracy = 0.949999988079071\n",
      "Epoch [145/500], Loss: 0.410514771938324 Training accuracy = 0.925000011920929 | Test accuracy = 0.949999988079071\n",
      "Epoch [146/500], Loss: 0.40936899185180664 Training accuracy = 0.925000011920929 | Test accuracy = 0.949999988079071\n",
      "Epoch [147/500], Loss: 0.4082295894622803 Training accuracy = 0.925000011920929 | Test accuracy = 0.949999988079071\n",
      "Epoch [148/500], Loss: 0.4070965349674225 Training accuracy = 0.925000011920929 | Test accuracy = 0.949999988079071\n",
      "Epoch [149/500], Loss: 0.4059697985649109 Training accuracy = 0.925000011920929 | Test accuracy = 0.949999988079071\n",
      "Epoch [150/500], Loss: 0.40484943985939026 Training accuracy = 0.925000011920929 | Test accuracy = 0.949999988079071\n",
      "Epoch [151/500], Loss: 0.4037353992462158 Training accuracy = 0.925000011920929 | Test accuracy = 0.949999988079071\n",
      "Epoch [152/500], Loss: 0.4026276469230652 Training accuracy = 0.925000011920929 | Test accuracy = 0.949999988079071\n",
      "Epoch [153/500], Loss: 0.4015261232852936 Training accuracy = 0.925000011920929 | Test accuracy = 0.949999988079071\n",
      "Epoch [154/500], Loss: 0.4004308581352234 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [155/500], Loss: 0.3993418216705322 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [156/500], Loss: 0.3982589840888977 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [157/500], Loss: 0.39718231558799744 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [158/500], Loss: 0.3961118757724762 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [159/500], Loss: 0.39504748582839966 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [160/500], Loss: 0.393989235162735 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [161/500], Loss: 0.3929370641708374 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [162/500], Loss: 0.3918909430503845 Training accuracy = 0.9375 | Test accuracy = 0.949999988079071\n",
      "Epoch [163/500], Loss: 0.3908509314060211 Training accuracy = 0.9375 | Test accuracy = 0.949999988079071\n",
      "Epoch [164/500], Loss: 0.38981685042381287 Training accuracy = 0.9375 | Test accuracy = 0.949999988079071\n",
      "Epoch [165/500], Loss: 0.38878875970840454 Training accuracy = 0.9375 | Test accuracy = 0.949999988079071\n",
      "Epoch [166/500], Loss: 0.387766569852829 Training accuracy = 0.9375 | Test accuracy = 0.949999988079071\n",
      "Epoch [167/500], Loss: 0.38675040006637573 Training accuracy = 0.9375 | Test accuracy = 0.949999988079071\n",
      "Epoch [168/500], Loss: 0.38574010133743286 Training accuracy = 0.9375 | Test accuracy = 0.949999988079071\n",
      "Epoch [169/500], Loss: 0.38473567366600037 Training accuracy = 0.9375 | Test accuracy = 0.949999988079071\n",
      "Epoch [170/500], Loss: 0.38373705744743347 Training accuracy = 0.9375 | Test accuracy = 0.949999988079071\n",
      "Epoch [171/500], Loss: 0.38274431228637695 Training accuracy = 0.9375 | Test accuracy = 0.949999988079071\n",
      "Epoch [172/500], Loss: 0.38175734877586365 Training accuracy = 0.9375 | Test accuracy = 0.949999988079071\n",
      "Epoch [173/500], Loss: 0.3807761073112488 Training accuracy = 0.925000011920929 | Test accuracy = 0.949999988079071\n",
      "Epoch [174/500], Loss: 0.37980061769485474 Training accuracy = 0.925000011920929 | Test accuracy = 0.949999988079071\n",
      "Epoch [175/500], Loss: 0.37883082032203674 Training accuracy = 0.925000011920929 | Test accuracy = 0.949999988079071\n",
      "Epoch [176/500], Loss: 0.3778666853904724 Training accuracy = 0.925000011920929 | Test accuracy = 0.949999988079071\n",
      "Epoch [177/500], Loss: 0.37690821290016174 Training accuracy = 0.925000011920929 | Test accuracy = 0.949999988079071\n",
      "Epoch [178/500], Loss: 0.3759552836418152 Training accuracy = 0.925000011920929 | Test accuracy = 0.949999988079071\n",
      "Epoch [179/500], Loss: 0.3750079870223999 Training accuracy = 0.925000011920929 | Test accuracy = 0.949999988079071\n",
      "Epoch [180/500], Loss: 0.3740662634372711 Training accuracy = 0.925000011920929 | Test accuracy = 0.949999988079071\n",
      "Epoch [181/500], Loss: 0.3731299936771393 Training accuracy = 0.925000011920929 | Test accuracy = 0.949999988079071\n",
      "Epoch [182/500], Loss: 0.37219929695129395 Training accuracy = 0.925000011920929 | Test accuracy = 0.949999988079071\n",
      "Epoch [183/500], Loss: 0.3712739944458008 Training accuracy = 0.925000011920929 | Test accuracy = 0.949999988079071\n",
      "Epoch [184/500], Loss: 0.3703541159629822 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [185/500], Loss: 0.3694397211074829 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [186/500], Loss: 0.3685305714607239 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [187/500], Loss: 0.367626816034317 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [188/500], Loss: 0.36672839522361755 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [189/500], Loss: 0.36583518981933594 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [190/500], Loss: 0.36494725942611694 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [191/500], Loss: 0.3640645146369934 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [192/500], Loss: 0.3631869852542877 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [193/500], Loss: 0.3623145520687103 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [194/500], Loss: 0.361447274684906 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [195/500], Loss: 0.36058509349823 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [196/500], Loss: 0.3597279191017151 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [197/500], Loss: 0.3588758111000061 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [198/500], Loss: 0.35802868008613586 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [199/500], Loss: 0.3571864664554596 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [200/500], Loss: 0.35634922981262207 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [201/500], Loss: 0.35551685094833374 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [202/500], Loss: 0.3546893298625946 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [203/500], Loss: 0.3538667559623718 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [204/500], Loss: 0.3530488908290863 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [205/500], Loss: 0.3522357940673828 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [206/500], Loss: 0.35142746567726135 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [207/500], Loss: 0.35062381625175476 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [208/500], Loss: 0.34982484579086304 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [209/500], Loss: 0.3490305542945862 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [210/500], Loss: 0.3482408821582794 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [211/500], Loss: 0.34745579957962036 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [212/500], Loss: 0.34667524695396423 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [213/500], Loss: 0.3458992540836334 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [214/500], Loss: 0.34512776136398315 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [215/500], Loss: 0.34436070919036865 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [216/500], Loss: 0.3435981273651123 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [217/500], Loss: 0.34283992648124695 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [218/500], Loss: 0.34208613634109497 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [219/500], Loss: 0.3413366377353668 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [220/500], Loss: 0.3405914902687073 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [221/500], Loss: 0.33985060453414917 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [222/500], Loss: 0.3391140401363373 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [223/500], Loss: 0.33838167786598206 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [224/500], Loss: 0.3376534879207611 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [225/500], Loss: 0.33692947030067444 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [226/500], Loss: 0.33620962500572205 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [227/500], Loss: 0.33549395203590393 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [228/500], Loss: 0.33478230237960815 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [229/500], Loss: 0.3340746760368347 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [230/500], Loss: 0.3333711624145508 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [231/500], Loss: 0.332671582698822 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [232/500], Loss: 0.3319760262966156 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [233/500], Loss: 0.3312843441963196 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [234/500], Loss: 0.3305966258049011 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [235/500], Loss: 0.32991284132003784 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [236/500], Loss: 0.3292329013347626 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [237/500], Loss: 0.32855677604675293 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [238/500], Loss: 0.3278844654560089 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [239/500], Loss: 0.3272159695625305 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [240/500], Loss: 0.32655125856399536 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [241/500], Loss: 0.3258902132511139 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [242/500], Loss: 0.3252328634262085 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [243/500], Loss: 0.3245791792869568 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [244/500], Loss: 0.32392922043800354 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [245/500], Loss: 0.32328280806541443 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [246/500], Loss: 0.322640061378479 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [247/500], Loss: 0.3220009207725525 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [248/500], Loss: 0.32136520743370056 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [249/500], Loss: 0.32073310017585754 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [250/500], Loss: 0.3201045095920563 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [251/500], Loss: 0.319479376077652 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [252/500], Loss: 0.31885772943496704 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [253/500], Loss: 0.3182394206523895 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [254/500], Loss: 0.317624568939209 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [255/500], Loss: 0.31701311469078064 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [256/500], Loss: 0.3164049983024597 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [257/500], Loss: 0.3158002197742462 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [258/500], Loss: 0.31519871950149536 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [259/500], Loss: 0.31460049748420715 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [260/500], Loss: 0.314005583524704 Training accuracy = 0.9125000238418579 | Test accuracy = 0.949999988079071\n",
      "Epoch [261/500], Loss: 0.31341391801834106 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [262/500], Loss: 0.31282538175582886 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [263/500], Loss: 0.3122400939464569 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [264/500], Loss: 0.31165796518325806 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [265/500], Loss: 0.3110790252685547 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [266/500], Loss: 0.3105030953884125 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [267/500], Loss: 0.30993038415908813 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [268/500], Loss: 0.30936068296432495 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [269/500], Loss: 0.3087940812110901 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [270/500], Loss: 0.308230459690094 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [271/500], Loss: 0.30766987800598145 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [272/500], Loss: 0.30711227655410767 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [273/500], Loss: 0.30655771493911743 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [274/500], Loss: 0.3060060143470764 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [275/500], Loss: 0.30545729398727417 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [276/500], Loss: 0.30491143465042114 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [277/500], Loss: 0.3043684959411621 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [278/500], Loss: 0.30382847785949707 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [279/500], Loss: 0.3032912313938141 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [280/500], Loss: 0.3027568459510803 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [281/500], Loss: 0.3022252321243286 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [282/500], Loss: 0.30169644951820374 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [283/500], Loss: 0.30117037892341614 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [284/500], Loss: 0.3006470799446106 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [285/500], Loss: 0.3001265227794647 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [286/500], Loss: 0.29960867762565613 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [287/500], Loss: 0.2990935444831848 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [288/500], Loss: 0.29858097434043884 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [289/500], Loss: 0.2980711758136749 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [290/500], Loss: 0.29756394028663635 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [291/500], Loss: 0.2970593571662903 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [292/500], Loss: 0.29655736684799194 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [293/500], Loss: 0.29605790972709656 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [294/500], Loss: 0.2955610454082489 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [295/500], Loss: 0.2950667440891266 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [296/500], Loss: 0.29457491636276245 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [297/500], Loss: 0.29408568143844604 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [298/500], Loss: 0.29359883069992065 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [299/500], Loss: 0.29311448335647583 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [300/500], Loss: 0.2926326394081116 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [301/500], Loss: 0.29215317964553833 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [302/500], Loss: 0.2916761338710785 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [303/500], Loss: 0.29120153188705444 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [304/500], Loss: 0.290729284286499 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [305/500], Loss: 0.2902594208717346 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [306/500], Loss: 0.28979191184043884 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [307/500], Loss: 0.2893267273902893 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [308/500], Loss: 0.2888638973236084 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [309/500], Loss: 0.28840333223342896 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [310/500], Loss: 0.287945032119751 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [311/500], Loss: 0.2874890863895416 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [312/500], Loss: 0.2870352864265442 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [313/500], Loss: 0.2865838408470154 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [314/500], Loss: 0.2861345410346985 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [315/500], Loss: 0.28568747639656067 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [316/500], Loss: 0.28524258732795715 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [317/500], Loss: 0.2847999036312103 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [318/500], Loss: 0.2843593955039978 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [319/500], Loss: 0.2839210331439972 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [320/500], Loss: 0.2834847569465637 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [321/500], Loss: 0.28305068612098694 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [322/500], Loss: 0.28261861205101013 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [323/500], Loss: 0.2821887135505676 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [324/500], Loss: 0.2817608714103699 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [325/500], Loss: 0.2813350558280945 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [326/500], Loss: 0.2809113562107086 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [327/500], Loss: 0.28048965334892273 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [328/500], Loss: 0.2800699770450592 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [329/500], Loss: 0.27965229749679565 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [330/500], Loss: 0.2792366147041321 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [331/500], Loss: 0.2788228690624237 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [332/500], Loss: 0.27841120958328247 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [333/500], Loss: 0.2780013978481293 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [334/500], Loss: 0.27759355306625366 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [335/500], Loss: 0.27718764543533325 Training accuracy = 0.9125000238418579 | Test accuracy = 1.0\n",
      "Epoch [336/500], Loss: 0.27678364515304565 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [337/500], Loss: 0.27638155221939087 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [338/500], Loss: 0.2759813666343689 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [339/500], Loss: 0.2755829989910126 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [340/500], Loss: 0.27518653869628906 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [341/500], Loss: 0.274791955947876 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [342/500], Loss: 0.27439916133880615 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [343/500], Loss: 0.274008184671402 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [344/500], Loss: 0.2736190855503082 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [345/500], Loss: 0.2732316851615906 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [346/500], Loss: 0.27284616231918335 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [347/500], Loss: 0.272462397813797 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [348/500], Loss: 0.2720804214477539 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [349/500], Loss: 0.2717001438140869 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [350/500], Loss: 0.2713216245174408 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [351/500], Loss: 0.27094486355781555 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [352/500], Loss: 0.2705698013305664 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [353/500], Loss: 0.27019646763801575 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [354/500], Loss: 0.2698248028755188 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [355/500], Loss: 0.26945483684539795 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [356/500], Loss: 0.2690865397453308 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [357/500], Loss: 0.26871994137763977 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [358/500], Loss: 0.26835495233535767 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [359/500], Loss: 0.2679916322231293 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [360/500], Loss: 0.2676299214363098 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [361/500], Loss: 0.26726987957954407 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [362/500], Loss: 0.26691141724586487 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [363/500], Loss: 0.2665545344352722 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [364/500], Loss: 0.2661992907524109 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [365/500], Loss: 0.26584556698799133 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [366/500], Loss: 0.2654934823513031 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [367/500], Loss: 0.26514285802841187 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [368/500], Loss: 0.26479387283325195 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [369/500], Loss: 0.2644464373588562 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [370/500], Loss: 0.26410046219825745 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [371/500], Loss: 0.26375603675842285 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [372/500], Loss: 0.26341310143470764 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [373/500], Loss: 0.2630716860294342 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [374/500], Loss: 0.26273176074028015 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [375/500], Loss: 0.2623932957649231 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [376/500], Loss: 0.2620563507080078 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [377/500], Loss: 0.26172080636024475 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [378/500], Loss: 0.2613867521286011 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [379/500], Loss: 0.2610541582107544 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [380/500], Loss: 0.26072293519973755 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [381/500], Loss: 0.2603932023048401 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [382/500], Loss: 0.26006489992141724 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [383/500], Loss: 0.25973790884017944 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [384/500], Loss: 0.25941237807273865 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [385/500], Loss: 0.2590882182121277 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [386/500], Loss: 0.25876548886299133 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [387/500], Loss: 0.25844407081604004 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [388/500], Loss: 0.25812405347824097 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [389/500], Loss: 0.25780540704727173 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [390/500], Loss: 0.2574880123138428 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [391/500], Loss: 0.2571720480918884 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [392/500], Loss: 0.25685739517211914 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [393/500], Loss: 0.2565440535545349 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [394/500], Loss: 0.25623205304145813 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [395/500], Loss: 0.2559213638305664 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [396/500], Loss: 0.2556118965148926 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [397/500], Loss: 0.2553037405014038 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [398/500], Loss: 0.2549969255924225 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [399/500], Loss: 0.25469133257865906 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [400/500], Loss: 0.2543869912624359 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [401/500], Loss: 0.2540839612483978 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [402/500], Loss: 0.25378215312957764 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [403/500], Loss: 0.2534816265106201 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [404/500], Loss: 0.2531822621822357 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [405/500], Loss: 0.2528842091560364 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [406/500], Loss: 0.25258728861808777 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [407/500], Loss: 0.25229161977767944 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [408/500], Loss: 0.251997172832489 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [409/500], Loss: 0.2517038881778717 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [410/500], Loss: 0.2514118254184723 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [411/500], Loss: 0.2511209547519684 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [412/500], Loss: 0.2508312463760376 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [413/500], Loss: 0.25054270029067993 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [414/500], Loss: 0.2502553164958954 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [415/500], Loss: 0.24996908009052277 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [416/500], Loss: 0.24968397617340088 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [417/500], Loss: 0.2494000643491745 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [418/500], Loss: 0.24911725521087646 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [419/500], Loss: 0.24883556365966797 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [420/500], Loss: 0.2485550194978714 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [421/500], Loss: 0.24827556312084198 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [422/500], Loss: 0.2479972392320633 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [423/500], Loss: 0.24772000312805176 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [424/500], Loss: 0.24744386970996857 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [425/500], Loss: 0.24716880917549133 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [426/500], Loss: 0.24689486622810364 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [427/500], Loss: 0.2466219663619995 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [428/500], Loss: 0.24635009467601776 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [429/500], Loss: 0.24607935547828674 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [430/500], Loss: 0.2458096742630005 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [431/500], Loss: 0.245541051030159 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [432/500], Loss: 0.2452733963727951 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [433/500], Loss: 0.24500684440135956 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [434/500], Loss: 0.244741290807724 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [435/500], Loss: 0.2444767951965332 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [436/500], Loss: 0.2442132979631424 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [437/500], Loss: 0.24395079910755157 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [438/500], Loss: 0.24368932843208313 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [439/500], Loss: 0.24342887103557587 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [440/500], Loss: 0.2431693971157074 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [441/500], Loss: 0.24291089177131653 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [442/500], Loss: 0.24265341460704803 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [443/500], Loss: 0.24239692091941833 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [444/500], Loss: 0.24214139580726624 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [445/500], Loss: 0.24188680946826935 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [446/500], Loss: 0.24163322150707245 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [447/500], Loss: 0.24138054251670837 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [448/500], Loss: 0.24112887680530548 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [449/500], Loss: 0.24087810516357422 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [450/500], Loss: 0.24062831699848175 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [451/500], Loss: 0.24037940800189972 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [452/500], Loss: 0.2401314675807953 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [453/500], Loss: 0.2398844212293625 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [454/500], Loss: 0.2396383285522461 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [455/500], Loss: 0.23939314484596252 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [456/500], Loss: 0.23914888501167297 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [457/500], Loss: 0.23890550434589386 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [458/500], Loss: 0.23866300284862518 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [459/500], Loss: 0.23842141032218933 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [460/500], Loss: 0.2381807565689087 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [461/500], Loss: 0.2379409521818161 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [462/500], Loss: 0.23770204186439514 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [463/500], Loss: 0.23746398091316223 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [464/500], Loss: 0.23722679913043976 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [465/500], Loss: 0.23699048161506653 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [466/500], Loss: 0.23675504326820374 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [467/500], Loss: 0.2365204393863678 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [468/500], Loss: 0.23628666996955872 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [469/500], Loss: 0.23605379462242126 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [470/500], Loss: 0.23582172393798828 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [471/500], Loss: 0.23559048771858215 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [472/500], Loss: 0.23536011576652527 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [473/500], Loss: 0.23513054847717285 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [474/500], Loss: 0.23490183055400848 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [475/500], Loss: 0.23467382788658142 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [476/500], Loss: 0.23444676399230957 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [477/500], Loss: 0.2342204600572586 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [478/500], Loss: 0.2339949607849121 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [479/500], Loss: 0.23377028107643127 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [480/500], Loss: 0.23354637622833252 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [481/500], Loss: 0.23332329094409943 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [482/500], Loss: 0.23310096561908722 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [483/500], Loss: 0.23287947475910187 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [484/500], Loss: 0.23265866935253143 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [485/500], Loss: 0.23243868350982666 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [486/500], Loss: 0.23221953213214874 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [487/500], Loss: 0.23200109601020813 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [488/500], Loss: 0.2317834198474884 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [489/500], Loss: 0.23156650364398956 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [490/500], Loss: 0.2313503473997116 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [491/500], Loss: 0.23113492131233215 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [492/500], Loss: 0.23092027008533478 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [493/500], Loss: 0.2307063341140747 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [494/500], Loss: 0.23049315810203552 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [495/500], Loss: 0.23028072714805603 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [496/500], Loss: 0.23006901144981384 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [497/500], Loss: 0.22985801100730896 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [498/500], Loss: 0.22964778542518616 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [499/500], Loss: 0.22943821549415588 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Epoch [500/500], Loss: 0.2292293757200241 Training accuracy = 0.925000011920929 | Test accuracy = 1.0\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "lr = 0.05\n",
    "num_epochs = 500\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "train = train.float()\n",
    "test = test.float()\n",
    "labels_train = labels_train.float()\n",
    "labels_test = labels_test.float()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model.forward(train)\n",
    "    loss = loss_fn(outputs, labels_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Calculate training and validation accuracy every epoch\n",
    "    training_accuracy = 0\n",
    "    test_accuracy = 0\n",
    "     \n",
    "    model.eval()\n",
    "    outputs = model.predict(train)\n",
    "    training_accuracy = (outputs == labels_train).float().mean()\n",
    "    outputs = model.predict(test)\n",
    "    test_accuracy = (outputs == labels_test).float().mean()\n",
    "    #print('Training accuracy = {} | Test accuracy = {}'.format(training_accuracy, test_accuracy))\n",
    "    \n",
    "    # Print the loss every epoch\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}' + ' Training accuracy = {} | Test accuracy = {}'.format(training_accuracy, test_accuracy))\n",
    "        \n",
    "print('Done!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  versicolor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_239047/3509775709.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_input = torch.tensor(test[idx], dtype=torch.float32).reshape(1, -1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "explainer = anchor_tabular.AnchorTabularExplainer(\n",
    "    target_names,\n",
    "    feature_names,\n",
    "    train.numpy()) \n",
    "\n",
    "def predict_fn(x):\n",
    "    return model.predict(torch.tensor(x).float()).numpy().astype(int)\n",
    "    \n",
    "\n",
    "\n",
    "idx = 0\n",
    "with torch.no_grad():\n",
    "    test_input = torch.tensor(test[idx], dtype=torch.float32).reshape(1, -1)\n",
    "    predicted_class = model.predict(test_input)\n",
    "    print('Prediction: ', explainer.class_names[int(predicted_class[0].item())])  # Access item directly and cast to int\n",
    "    \n",
    "    # Convert test_input to a NumPy array before passing it to explain_instance\n",
    "    test_input_np = test_input.numpy()[0]\n",
    "    \n",
    "    # Define a function to predict using the model and return NumPy array\n",
    "    def predict_fn(x):\n",
    "        return model.predict(torch.tensor(x).float()).numpy().astype(int).reshape(-1)\n",
    "    \n",
    "    # Pass the predict function to explain_instance\n",
    "    exp = explainer.explain_instance(test_input_np, predict_fn, threshold=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: petal length (cm) > 5.50\n",
      "Precision: 1.00\n",
      "Coverage: 0.22\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Anchor: %s' % (' AND '.join(exp.names())))\n",
    "print('Precision: %.2f' % exp.precision())\n",
    "print('Coverage: %.2f' % exp.coverage())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tnevenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
