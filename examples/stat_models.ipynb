{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of creating data, preprocess with noise and train statistical models\n",
    "In this example we will train statistical models on node classification. To this end, we will first generate a transaction log with Swish-AMLsim. Then split the data into two parts: a trainset and a testset. The split is done in time with a overlap. For each set we then build a graph of nodes and edges. The nodes represent the bank accounts and the edges represent reletions between two accounts. The nodes and edges will have aggregated features of the transactions and the labels will be on the nodes. The label is negative (0) if the node didn't participate in money laundering acitivities and positive (1) otherwise. \n",
    "\n",
    "The features on the nodes are:\n",
    "- sums, means, medians, stds, maxs, mins, counts of spending transactions (transactions to the sink).\n",
    "- sums, means, medians, stds, maxs, mins of swish transactions (transactions within the network).\n",
    "- number of incoming and outgoing swish transactions.\n",
    "- number of unique accounts the node has sent to and recived by.\n",
    "\n",
    "The features of the edges are:\n",
    "- sums, means, medians, stds, maxs, mins, counts of transactions between the two nodes.\n",
    "\n",
    "The labels are:\n",
    "- 0 if the node didn't participate in money laundering acitivities.\n",
    "- 1 if the node participated in money laundering acitivities. \n",
    "\n",
    "After building the graph, we will add noise to the labels in different fashion. In this example, there are four types of noise:\n",
    "- Flipped labels: random nodes flip their labels.\n",
    "- Misssing labels: random nodes will lose their label.\n",
    "- Neighbors noise: random negative nodes will have their labels flipped if they have a positive neighbor.\n",
    "- Topological noise: random nodes in specific topologies will have their labels flipped, suggesting a bank missing labels for specific money laundering stratergies.\n",
    "We will create four different datasets, one for each type of noise.\n",
    "\n",
    "Finally, we will train statistical models on the noisy data and evaluate the performance of the model on the test set. The models are:\n",
    "- Logistic regression\n",
    "- Random forest\n",
    "- Gradient boosting\n",
    "- Support vector machine\n",
    "- K-nearest neighbors\n",
    "\n",
    "The result are TN, FP, FN and TP for 101 threshold between 0 and 1. These can be used calculate wanted matics like ROC, AUC, F1, precision, recall, and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "\n",
    "from preprocess.feature_engineering import *\n",
    "from preprocess.noise import *\n",
    "from benchmark.stat_models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data\n",
    "TODO: write code to run AMLsim from a notebook.\n",
    "\n",
    "For now we will use a pre-generated dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = '10K_accts'\n",
    "dataset_path = f'../AMLsim/outputs/10K_accts/tx_log.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(dataset_path)\n",
    "bank = 'bank'\n",
    "overlap = 0.5 # overlap of training and testing data\n",
    "windows = [(0, 30)] # int or list of tuples - if int then the number of windows, if list of tuples then the start and end step for each window\n",
    "\n",
    "df_bank = df[(df['bankOrig'] == bank) | (df['bankDest'] == bank)]\n",
    "\n",
    "train_start = df_bank['step'].min()\n",
    "train_end = df_bank['step'].min() + (df_bank['step'].max() - df_bank['step'].min()) * (overlap+(1-overlap)/2)\n",
    "\n",
    "test_start = df_bank['step'].min() + (df_bank['step'].max() - df_bank['step'].min()) * (1-overlap)/2\n",
    "test_end = df_bank['step'].max()\n",
    "\n",
    "df_bank_train = df_bank[(df_bank['step'] >= train_start) & (df_bank['step'] <= train_end)]\n",
    "df_bank_test = df_bank[(df_bank['step'] >= test_start) & (df_bank['step'] <= test_end)]\n",
    "\n",
    "df_nodes_train = get_nodes(df_bank_train, bank, windows)\n",
    "df_edges_train = get_edges(df_bank_train[(df_bank_train['bankOrig'] == bank) & (df_bank_train['bankDest'] == bank)], windows, aggregated=True, directional=True) # TODO: enable edges to/from the bank? the node features use these txs but unclear how to ceate a edge in this case, the edge can't be connected to a node with node features (could create node features based on edge txs, then the node features and edge features will look the same and some node features will be missing)\n",
    "\n",
    "df_nodes_test = get_nodes(df_bank_test, bank, windows)\n",
    "df_edges_test = get_edges(df_bank_test[(df_bank_test['bankOrig'] == bank) & (df_bank_test['bankDest'] == bank)], windows, aggregated=True, directional=True)\n",
    "\n",
    "df_nodes_train.reset_index(inplace=True)\n",
    "node_to_index = pd.Series(df_nodes_train.index, index=df_nodes_train['account']).to_dict()\n",
    "\n",
    "df_edges_train['src'] = df_edges_train['src'].map(node_to_index) # OBS: in the csv files it looks like the edge src refers to the node two rows above the acculat node, this is due to the column head and that it starts counting at 0\n",
    "df_edges_train['dst'] = df_edges_train['dst'].map(node_to_index)\n",
    "\n",
    "df_nodes_test.reset_index(inplace=True)\n",
    "node_to_index = pd.Series(df_nodes_test.index, index=df_nodes_test['account']).to_dict()\n",
    "\n",
    "df_edges_test['src'] = df_edges_test['src'].map(node_to_index)\n",
    "df_edges_test['dst'] = df_edges_test['dst'].map(node_to_index)\n",
    "\n",
    "print('\\ntrainset:\\nnodes:')\n",
    "display(df_nodes_train.head())\n",
    "print('edges:')\n",
    "display(df_edges_train.head())\n",
    "print('\\ntestset:\\nnodes:')\n",
    "display(df_nodes_test.head())\n",
    "print('edges:')\n",
    "display(df_edges_test.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add noise to the labels\n",
    "\n",
    "### Flipped labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0, 1] # labels that will be affected by noise\n",
    "fracs = [0.01, 0.1] # fractions of nodes that will be affected by noise for each label\n",
    "df_flipped_lables = flip_labels(df_nodes_train, labels, fracs)\n",
    "display(df_flipped_lables.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0, 1] # labels that will be affected by noise\n",
    "fracs = [0.01, 0.1] # fractions of nodes that will be affected by noise for each label\n",
    "df_missing_labels = missing_labels(df_nodes_train, labels, fracs)\n",
    "display(df_missing_labels.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neighbour noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbour_frac = 0.5 # fraction of negative neighbours that will be flipped to positive\n",
    "df_flipped_neighbours = flip_neighbours(df_nodes_train, df_edges_train, neighbour_frac)\n",
    "display(df_flipped_neighbours.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topology noise\n",
    "We will add noise to 'gather_scatter', 'scatter_gather', 'stack' while keeping fan_in, fan_out and bipartite untoched to see if the models can generalize to more complex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topologies = ['gather_scatter', 'scatter_gather', 'stack'] \n",
    "topology_frac = 0.5 # fraction of labels in the topologies that will be affected by noise\n",
    "\n",
    "# to add the topology noise we need to know the alert members, this will be changed in the future\n",
    "alert_members = pd.read_csv(dataset_path.replace('outputs', 'tmp').replace('tx_log.csv', 'alert_members.csv'))\n",
    "\n",
    "df_flipped_topologies = topology_noise(df_nodes_train, alert_members, topologies, fracs[1])\n",
    "display(df_flipped_topologies.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models on the datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['XGB','RF','SVM','KNN','LOG']\n",
    "thresholds = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datasets = (df_nodes_train, df_nodes_test)\n",
    "results = train_models(datasets, model_names)\n",
    "for model_name, metrics in results.items():\n",
    "    print(model_name)\n",
    "    for metric, value in metrics.items():\n",
    "        if metric in thresholds:\n",
    "            print(f'  threshold: {metric}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (df_flipped_lables, df_nodes_test) \n",
    "results = train_models(datasets, model_names)\n",
    "for model_name, metrics in results.items():\n",
    "    print(model_name)\n",
    "    for metric, value in metrics.items():\n",
    "        if metric in thresholds:\n",
    "            print(f'  threshold: {metric}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (df_missing_labels, df_nodes_test) \n",
    "results = train_models(datasets, model_names)\n",
    "for model_name, metrics in results.items():\n",
    "    print(model_name)\n",
    "    for metric, value in metrics.items():\n",
    "        if metric in thresholds:\n",
    "            print(f'  threshold: {metric}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (df_flipped_neighbours, df_nodes_test)\n",
    "results = train_models(datasets, model_names)\n",
    "for model_name, metrics in results.items():\n",
    "    print(model_name)\n",
    "    for metric, value in metrics.items():\n",
    "        if metric in thresholds:\n",
    "            print(f'  threshold: {metric}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (df_flipped_topologies, df_nodes_test)\n",
    "results = train_models(datasets, model_names)\n",
    "for model_name, metrics in results.items():\n",
    "    print(model_name)\n",
    "    for metric, value in metrics.items():\n",
    "        if metric in thresholds:\n",
    "            print(f'  threshold: {metric}: {value}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
